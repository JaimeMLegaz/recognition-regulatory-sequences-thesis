---
title: "Training the network of the UTR experiment"
author: "Jaime MartÃ­nez Legaz"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    pdf_document: default
    toc_depth: 3
    number_sections: true
    theme: lumen
---

# Introduction

In this experiment we will try to answer the following question: Should we apply padding to our sequences? To do that, we have to build two groups of networks: One of them has networks trained only with sequences with padding, and the other group with sequences with no padding. 

After that, we will only have to evaluate both groups of networks, and then compare results.

```{r, include=FALSE}
library(biomaRt)
library(SetupSequences)
library(keras)
library(caret)
library(tidyverse)
library(BSgenome)
library(BSgenome.Hsapiens.UCSC.hg38.masked)
library(GenomicRanges)
```

# Padding experiment

## Obtaining the sequences

We have to obtain the sequences we will work with. This is the code that will achieve that.

```{r}
ml_table = readRDS(file = "../../../Data/ml_data_rf.rds") %>% 
  dplyr::select(class) %>% 
  rownames_to_column("id") %>% 
  tidyr::separate(id, c("gene", "seqnames", "start", "end", "strand", "tmp"), sep=":", remove=FALSE)
  

# converting to Granges object
ml_table_gr = makeGRangesFromDataFrame(ml_table, keep.extra.columns = TRUE)

# adding "chr" in front of seqnames
newStyle <- mapSeqlevels(seqlevels(ml_table_gr), "UCSC")
ml_table_gr <- renameSeqlevels(ml_table_gr, newStyle)

# Extract sequences using the package BSgenome
data = data.frame(
  id = ml_table$id,
  class = ml_table$class,
  sequence = BSgenome::getSeq(Hsapiens, ml_table_gr, as.character = TRUE) #as.character=TRUE does not work?
)
data$sequence <- as.character(data$sequence)
table(data$class)
```

Next step is grouping all the non 3'UTR sequences into a group, so that we can compare 3'UTR sequences with non 3'UTR sequences regardless of the subtype.

```{r}
old_data <- data # Needed for the valdation tests

data <- data[data$class == "ICE" | data$class == "UTR",]
data$detailclass = data$class
data$class = ifelse(data$class %in% "UTR", "UTR", "Non_3_UTR")  %>% as.factor() %>% as.integer()
data$class <- data$class - 1 #0 = No3UTR 1 = 3UTR
table(data$class)
```

```{r}
# Validation test
test1 = table(data$class)["0"] == nrow(data) - table(old_data$class)["UTR"]; if (test1 == FALSE) print ("ERROR 1")
test2 = table(data$class)["1"] == table(old_data$class)["UTR"]; if (test2 == FALSE) print ("ERROR 2")

```

We will separate these sequences into the "Positive" group (3'UTR) and the "Negative" group (non-3'UTR).

```{r}
PositiveSequences = data[data$class == 1,]
NegativeSequences = data[data$class == 0,]
```


## Treating the sequences

First, we recover the sequences we obtained with ensembl.

```{r}
PosSequences <- PositiveSequences
NegSequences <- NegativeSequences
```

Next we start with the treatment of sequences. We have to:

- Reverse them
- Make sure all sequences are 250 characters long
- One-hot encode them

For that, we need some functions from the custom package developed for this project.

```{r}
# We save the sequences for later use
PosSequences$rawseq <- PosSequences$sequence
NegSequences$rawseq <- NegSequences$sequence

# Example of sequence: AACCGT
PosSequences$sequence[1]  # Example of a positive sequence
NegSequences$sequence[1]  # Example of a negative sequence
```


```{r}
old_seq = PosSequences$sequence[1]

# strReverse:  AACCGT --> TGCCAA
PosSequences$sequence <- strReverse(PosSequences$sequence)
NegSequences$sequence <- strReverse(NegSequences$sequence)

PosSequences$sequence[1]  # The positive sequence, once reversed
NegSequences$sequence[1]  # The negative sequence, once reversed

# Validation
doublereverse = strReverse(PosSequences$sequence[1])
test1 = doublereverse == old_seq; if (test1 == FALSE) print ("ERROR 1")
```

```{r}
# padding_sequences: Inserts padding characters ("X") in sequences shorter than 250 characters, and trims sequences longer than 250 characters
PosSequences$sequence <- padding_sequences(PosSequences$sequence)
NegSequences$sequence <- padding_sequences(NegSequences$sequence)

PosSequences$sequence[1]  # The positive sequence, with some characters added so it can be 250 characters long
NegSequences$sequence[1]  # The negative sequence, trimmed so it becomes 250 characters long

# Validation
test1 = all(sapply(list(min(nchar(PosSequences$sequence)),max(nchar(PosSequences$sequence)), mean(nchar(PosSequences$sequence))), function(x) x == 250))
test2 = all(sapply(list(min(nchar(NegSequences$sequence)),max(nchar(NegSequences$sequence)), mean(nchar(NegSequences$sequence))), function(x) x == 250))
if (test1 == FALSE) print ("ERROR 1")
if (test2 == FALSE) print ("ERROR 2")
```

```{r}
# to_onehot:   TGCCAA --> 00010010010010001000    (A = 1000, C = 0100, G = 0010, T = 0001)
PosSequences$sequence <- padding_onehot(PosSequences$sequence)
NegSequences$sequence <- padding_onehot(NegSequences$sequence)

PosSequences$sequence[1]  # The positive sequence, reversed, padded and one-hot encoded
NegSequences$sequence[1]  # The negative sequence, reversed, trimmed and one-hot encoded

# Validation
test1 = all(sapply(list(min(nchar(PosSequences$sequence)),max(nchar(PosSequences$sequence)), mean(nchar(PosSequences$sequence))), function(x) x == 1250))
test2 = all(sapply(list(min(nchar(NegSequences$sequence)),max(nchar(NegSequences$sequence)), mean(nchar(NegSequences$sequence))), function(x) x == 1250))
if (test1 == FALSE) print ("ERROR 1")
if (test2 == FALSE) print ("ERROR 2")
```

```{r}
# Final check: Since some of the sequences might be corrupt, we will delete the ones that are corrupt, if they exist
# We can know if a sequence is corrupt by looking for the W character. When one-hot encoding, we encoded everything that was not an A, T, C or G with a "W"

if (any(grepl("W",PosSequences$sequence))){
  print("Found 'W'")
  PosSequences <- PosSequences[-which(grepl("W",PosSequences$sequence)),]
}
  
if (any(grepl("W",NegSequences$sequence))){
  print("Found 'W'")
  NegSequences <- NegSequences[-which(grepl("W",NegSequences$sequence)),]
}
```

Once treated, we can save them in a file for later use.

```{r, eval=FALSE, echo=FALSE}
fileCon<-file("../Data/Pos3UTR")
write.table(PosSequences$sequence,file = fileCon, quote=FALSE, row.names = FALSE, col.names = FALSE)
fileCon<-file("../Data/Neg3UTR")
write.table(NegSequences$sequence,file = fileCon, quote=FALSE, row.names = FALSE, col.names = FALSE)
```

## Training the network

First we have to recover the treated sequences.

```{r}
PosUTR <- PosSequences$sequence
length(PosUTR)
PosSequences_nodup <- PosSequences[-which(duplicated(PosUTR)),]
PosUTR <- PosUTR[-which(duplicated(PosUTR))]
length(PosUTR)

NegUTR <- NegSequences$sequence
length(NegUTR)
NegSequences_nodup <- NegSequences[-which(duplicated(NegUTR)),]
NegUTR <- NegUTR[-which(duplicated(NegUTR))]
length(NegUTR)

# Validation test
test1 = all(NegSequences_nodup$sequence == NegUTR) 
test2 = all(PosSequences_nodup$sequence == PosUTR)
if (test1 == FALSE) print ("ERROR 1")
if (test2 == FALSE) print ("ERROR 2")
```

Now we face an issue with our sequences. They are 1250 characters long strings, but R sees them as indivisible elements, not as vectors of 1250 elements. We have to transform them before working with them, turning each 1250 characters long sequence into 1250 sequences of only one character.

```{r}
posNum <- length(PosUTR) # Number of positive sequences
SeqUTR <- append(PosUTR,NegUTR)

old_seq = SeqUTR[1] # For validation tests

n2 <- nchar(SeqUTR[1]) -1 # Number of divisions to make

secuenciasOH <- sapply(1 + 1*0:n2, function(z) substr(SeqUTR, z, z))  # Obtaining the split characters
df <- data.frame(secuenciasOH, "Value" = 0) # Saving them into a dataframe
indx <- sapply(df, is.factor) 
df[indx] <- lapply(df[indx], function(x) as.character(x)) # Factor --> char conversion

df[1:posNum,]$Value <- 1 # We have the value of the positive sequences to positive

table(df$Value)

# Validation tests
test1 = paste(unlist(secuenciasOH[1,]),sep="", collapse="") == old_seq; if (test1 == FALSE) print ("ERROR 1")
test2 = table(df$Value)["0"] == length(NegUTR); if (test2 == FALSE) print ("ERROR 2")
test3 = table(df$Value)["1"] == length(PosUTR); if (test3 == FALSE) print ("ERROR 3")
test4 = all(length(PosUTR) + length(NegUTR) == length(SeqUTR), length(SeqUTR) == nrow(df)); if (test4 == FALSE) print ("ERROR 4")
```

Since we are going to train five different models, we will divide our data into five different dataframes. Each of them will have the same number of positive and negative sequences. Since we don't have many positive sequences, we will train every model with every positive sequences. This way, every model will be trained with the same set of positive sequences, but a different set of negative sequences.

```{r}
# Select all the negative sequences
set.seed(1234)
chosenSeqs <- sample(nrow(df) - posNum, size = posNum*5, replace = FALSE, prob = NULL) # Selected rows from 0 to max-posNum
chosenSeqs <- chosenSeqs + posNum

sets <- list()

sets[[1]] <- 1:posNum
sets[[2]] <- (posNum+1):(posNum*2)
sets[[3]] <- ((posNum*2)+1):(posNum*3)
sets[[4]] <- ((posNum*3)+1):(posNum*4)
sets[[5]] <- ((posNum*4)+1):(posNum*5)

```

```{r}

df.a <- df[c(1:posNum,chosenSeqs[sets[[1]]]),]
df.b <- df[c(1:posNum,chosenSeqs[sets[[2]]]),]
df.c <- df[c(1:posNum,chosenSeqs[sets[[3]]]),]
df.d <- df[c(1:posNum,chosenSeqs[sets[[4]]]),]
df.e <- df[c(1:posNum,chosenSeqs[sets[[5]]]),]

dfs <- list(df.a, df.b, df.c, df.d, df.e) # 

totalSeq = (nrow(df.a) + nrow(df.b) + nrow(df.c) + nrow(df.d) + nrow(df.e)) 
length(SeqUTR) - totalSeq # Number of sequences that do not appear in the dataframes
test1 = all(sapply(list(table(df.a$Value),table(df.b$Value), table(df.c$Value), table(df.d$Value)), function(x) x == table(df.e$Value)))
test2 = table(df.a$Value)["0"] == posNum
test3 = table(df.a$Value)["1"] == posNum
if (test1) print ("Correct 1") else print ("ERROR 1")
if (test2) print ("Correct 2") else print ("ERROR 2")
if (test3) print ("Correct 3") else print ("ERROR 3")

```

Now we have to divide our data into Training and Test for our model training.

```{r}

output <- c("Value")
trains <- list()
tests <- list()
usedForTrainPos <- list()
usedForTrainNeg <- list()

set.seed(1234)

for (i in 1:5){
  partition <- createDataPartition(dfs[[i]][[output]],
                                     p = 0.8,
                                     list = FALSE,
                                     times = 1)
  trains <- append(trains, list(dfs[[i]][partition,])) # 28210
  tests <- append(tests, list(dfs[[i]][-partition,])) # 7852
  # Sum of both: 35262

  partPos <- partition[partition <= posNum]
  partNeg <- partition[partition > posNum]
  partNeg <- partNeg - posNum
  
  usedForTrainPos <- append(usedForTrainPos,list(unlist(partPos)))
  usedForTrainNeg <- append(usedForTrainNeg,list(unlist(chosenSeqs[sets[[i]][partNeg]])))
}

# Validation test

total = 0

for (i in 1:5) {
  total = total + nrow(trains[[i]]) + nrow(tests[[i]])
}

test1 = total == totalSeq; if (test1 == FALSE) print ("ERROR 1")
test2 = sum(duplicated(usedForTrainPos)) == 0; if (test2 == FALSE) print ("ERROR 2")
test3 = sum(duplicated(usedForTrainNeg)) == 0; if (test3 == FALSE) print ("ERROR 3")

#usedForTrainPos <- usedForTrainPos[-which(duplicated(usedForTrainPos))] # Only the positive genes could be duplicated

```

Now we check the type distribution of the non-3'UTR sequences. We are interested in distinguishing 3'UTR from ICE sequences. So, the desired output would be 0 in every subtype except for ICE.

```{r}
# Checking the type distribution of the train sets

for (i in 1:5){
  negtr <- usedForTrainNeg[[i]]
 # print(min(negtr))
 # print(max(negtr))
  print(table(NegSequences_nodup$detailclass[negtr]))
}


```

Once divided the data, we have to adapt the data to the format the model expects.

```{r}
train.x <- list()
train.y <- list()

test.x <- list()
test.y <- list()

for (i in 1:5){
  train.x <- append(train.x, list(data.matrix(trains[[i]][,1:1250])))
  train.y <- append(train.y, list(data.matrix(trains[[i]][,1251])))
  
  test.x <- append(test.x, list(data.matrix(tests[[i]][,1:1250])))
  test.y <- append(test.y, list(data.matrix(tests[[i]][,1251])))

  train.x[[i]] <- array_reshape(train.x[[i]], c(nrow(trains[[i]]),1250,1))
  test.x[[i]] <- array_reshape(test.x[[i]], c(nrow(tests[[i]]),1250,1))
 }
```

We have to prepare the dataframe that will contain the sequences and the output of the models.

```{r}
# Next step is making a dataframe of every control gene and predicting positive or negative
genesN <- NegSequences_nodup
genesP <- PosSequences_nodup

 ##### We have to make some transformations to these secuences

n2 <- nchar(genesN$sequence[1]) - 1

secuencesOH <- sapply(1 + 1*0:n2, function(z) substr(genesN$sequence, z, z))  
dfn <- data.frame(secuencesOH, class=genesN$detailclass, id=genesN$id) 
indx <- sapply(dfn, is.factor) 
dfn[indx] <- lapply(dfn[indx], function(x) as.character(x))  #

secuencesOH <- sapply(1 + 1*0:n2, function(z) substr(genesP$sequence, z, z))  
dfp <- data.frame(secuencesOH, class=genesP$detailclass, id=genesP$id) 
indx <- sapply(dfp, is.factor) 
dfp[indx] <- lapply(dfp[indx], function(x) as.character(x))  

# We have got a dataframe with the ensembl_id, sequence and hgnc symbol in genesP and genesN, and a dataframe with the split sequence and hgnc symbol in dfp and dfn

finalSequencesN <- data.matrix(dfn[,1:1250])
finalSequencesN <- array_reshape(finalSequencesN,c(nrow(finalSequencesN),1250,1))
finalSequencesP <- data.matrix(dfp[,1:1250])
finalSequencesP <- array_reshape(finalSequencesP,c(nrow(finalSequencesP),1250,1))

genesP$model1 <- 0
genesP$model2 <- 0
genesP$model3 <- 0
genesP$model4 <- 0
genesP$model5 <- 0

genesN$model1 <- 0
genesN$model2 <- 0
genesN$model3 <- 0
genesN$model4 <- 0
genesN$model5 <- 0

```

Now it is time to build our model. It follows the specifications mentioned in the thesis report. 

```{r}
  batch_size <- 125
  epochs <- 25 
  input_shape <- c(1250,1)
  learn_rate = 0.0001
  
  modelConvolu <- keras_model_sequential()
  modelConvolu %>% 
    layer_conv_1d(filters = 48, kernel_size = 75, activation = "relu", input_shape = input_shape)%>%
    layer_flatten() %>%
    layer_dense(units = 50, activation = 'relu') %>% 
    layer_dense(units = 1, activation="sigmoid") %>%
    
    summary(model)
  
  modelConvolu %>% compile(
    loss = loss_binary_crossentropy,
    optimizer = optimizer_nadam(lr = learn_rate),
    metrics = c('accuracy')
  )
```



```{r, echo=FALSE}
config = tensorflow::tf$ConfigProto(gpu_options = list(allow_growth = TRUE))
sess = tensorflow::tf$Session(config = config)
keras::k_set_session(session = sess)
```

And now we train that model with the data obtained. After each training epoch, we will obtain certain statistics related to the sensitivity and specificity of the model, since they are not metrics that Keras can return from the model.

```{r}
#with(tf$device("/device:GPU:0"), {

models <- list()
all_history <- list()

for (i in 1:5){
  
  sens <- NULL
  spec <- NULL
  history <- NULL
 
  
   modelConvolu <- keras_model_sequential()
  modelConvolu %>% 
    layer_conv_1d(filters = 48, kernel_size = 75, activation = "relu", input_shape = input_shape)%>%
    layer_flatten() %>%
    layer_dense(units = 50, activation = 'relu') %>% 
    layer_dense(units = 1, activation="sigmoid") 
  
  modelConvolu %>% compile(
    loss = loss_binary_crossentropy,
    optimizer = optimizer_nadam(lr = learn_rate),
    metrics = c('accuracy')
  )
  
    historial <- NULL
  
    for (epoch in 1:epochs){
    historial <- modelConvolu %>% fit(
      x = train.x[[i]],
      y = train.y[[i]],
      epochs = 1,  
      batch_size = batch_size, 
      validation_data = list(test.x[[i]], test.y[[i]]),
      verbose = 2)
    
    history <- c(history, historial)
    
    ## Calculating sensitivity and specificity
    pred <- modelConvolu %>% predict(test.x[[i]], batch_size = batch_size)
    prediction.y = round(pred)
    # Confusion matrix
    CM = table(prediction.y, test.y[[i]])
    if (length(CM) > 2){
      sens <- c(sens,sensitivity(CM))
      spec <- c(spec,specificity(CM))
    }
    else {  # This happens in the weird cases the model only predicts positive or negative
      if (sum(grepl(0,prediction.y)) > 0){
        sens <- c(sens, 0)
        spec <- c(spec, 1)
      }
      else{
        sens <- c(sens, 0)
        spec <- c(spec, 1)
      }
    }
  }
  ## Prediction 
  pred <- modelConvolu %>% predict(finalSequencesP, batch_size = batch_size)
  genesP[,5+i] = pred
  
  pred <- modelConvolu %>% predict(finalSequencesN, batch_size = batch_size)
  genesN[,5+i] = pred
  
  models <- append(models, modelConvolu)
  all_history <- append(all_history, historial)
  
  
  
}

#})
```

## Results

Once trained the networks, we will sumarize the results, to later compare them with the other experiment:

```{r}

acc_p_neg <- sum(genesN[,6:10] < 0.5)/(nrow(genesN)*5)
acc_p_pos <- sum(genesP[,6:10] > 0.5)/(nrow(genesP)*5)
acc_p <- (acc_p_neg + acc_p_pos)/2

error_p_neg <- sum(genesN[,6:10])/(nrow(genesN)*5)
error_p_pos <- sum(1 - genesP[,6:10])/(nrow(genesP)*5)
error_p <- (error_p_neg + error_p_pos)/2

sens_p <- sens
spec_p <- spec

```

# No padding experiment

## Obtaining the sequences

We have to obtain the sequences we will work with. This is the code that will achieve that.

```{r}
ml_table = readRDS(file = "../../../Data/ml_data_rf.rds") %>% 
  dplyr::select(class) %>% 
  rownames_to_column("id") %>% 
  tidyr::separate(id, c("gene", "seqnames", "start", "end", "strand", "tmp"), sep=":", remove=FALSE)
  

# converting to Granges object
ml_table_gr = makeGRangesFromDataFrame(ml_table, keep.extra.columns = TRUE)

# adding "chr" in front of seqnames
newStyle <- mapSeqlevels(seqlevels(ml_table_gr), "UCSC")
ml_table_gr <- renameSeqlevels(ml_table_gr, newStyle)

# Extract sequences using the package BSgenome
data = data.frame(
  id = ml_table$id,
  class = ml_table$class,
  sequence = BSgenome::getSeq(Hsapiens, ml_table_gr, as.character = TRUE) #as.character=TRUE does not work?
)
data$sequence <- as.character(data$sequence)
table(data$class)
```

Next step is grouping all the non 3'UTR sequences into a group, so that we can compare 3'UTR sequences with non 3'UTR sequences regardless of the subtype.

```{r}
old_data <- data # Needed for the valdation tests

data <- data[data$class == "ICE" | data$class == "UTR",]
data$detailclass = data$class
data$class = ifelse(data$class %in% "UTR", "UTR", "Non_3_UTR")  %>% as.factor() %>% as.integer()
data$class <- data$class - 1 #0 = No3UTR 1 = 3UTR
table(data$class)
```

```{r}
# Validation test
test1 = table(data$class)["0"] == nrow(data) - table(old_data$class)["UTR"]; if (test1 == FALSE) print ("ERROR 1")
test2 = table(data$class)["1"] == table(old_data$class)["UTR"]; if (test2 == FALSE) print ("ERROR 2")

```

We will separate these sequences into the "Positive" group (3'UTR) and the "Negative" group (non-3'UTR).

```{r}
PositiveSequences = data[data$class == 1,]
NegativeSequences = data[data$class == 0,]
```


## Treating the sequences

First, we recover the sequences we obtained with ensembl.

```{r}
PosSequences <- PositiveSequences[nchar(PositiveSequences$sequence) >= 202,]
NegSequences <- NegativeSequences[nchar(NegativeSequences$sequence) >= 202,]

PosSequences_all <- PositiveSequences # These are the ones that are going to be evaluated
NegSequences_all <- NegativeSequences
```

Next we start with the treatment of sequences. We have to:

- Reverse them
- Make sure all sequences are 250 characters long
- One-hot encode them

For that, we need some functions from the custom package developed for this project.

```{r}
# We save the sequences for later use
PosSequences$rawseq <- PosSequences$sequence
NegSequences$rawseq <- NegSequences$sequence

PosSequences_all$rawseq <- PosSequences_all$sequence
NegSequences_all$rawseq <- NegSequences_all$sequence

# Example of sequence: AACCGT
PosSequences$sequence[1]  # Example of a positive sequence
NegSequences$sequence[1]  # Example of a negative sequence
```


```{r}
old_seq = PosSequences$sequence[1]

# strReverse:  AACCGT --> TGCCAA
PosSequences$sequence <- strReverse(PosSequences$sequence)
NegSequences$sequence <- strReverse(NegSequences$sequence)

PosSequences_all$sequence <- strReverse(PosSequences_all$sequence)
NegSequences_all$sequence <- strReverse(NegSequences_all$sequence)

PosSequences$sequence[1]  # The positive sequence, once reversed
NegSequences$sequence[1]  # The negative sequence, once reversed

# Validation
doublereverse = strReverse(PosSequences$sequence[1])
test1 = doublereverse == old_seq; if (test1 == FALSE) print ("ERROR 1")
```

```{r}
# padding_sequences: Inserts padding characters ("X") in sequences shorter than 250 characters, and trims sequences longer than 202 characters
PosSequences$sequence <- padding_sequences(PosSequences$sequence, length=202) # Check the Study_Length_Padding markdown to know why 202 length
NegSequences$sequence <- padding_sequences(NegSequences$sequence, length=202)

PosSequences_all$sequence <- padding_sequences(PosSequences_all$sequence, length=202)
NegSequences_all$sequence <- padding_sequences(NegSequences_all$sequence, length=202)

PosSequences$sequence[1]  # The positive sequence, with some characters added so it can be 202 characters long
NegSequences$sequence[1]  # The negative sequence, trimmed so it becomes 202 characters long

# Validation
test1 = all(sapply(list(min(nchar(PosSequences$sequence)),max(nchar(PosSequences$sequence)), mean(nchar(PosSequences$sequence))), function(x) x == 202))
test2 = all(sapply(list(min(nchar(NegSequences$sequence)),max(nchar(NegSequences$sequence)), mean(nchar(NegSequences$sequence))), function(x) x == 202))
if (test1 == FALSE) print ("ERROR 1")
if (test2 == FALSE) print ("ERROR 2")
```

```{r}
# Final check: Since some of the sequences might be corrupt, we will delete the ones that are corrupt, if they exist
# We can know if a sequence is corrupt by looking for the W character. When one-hot encoding, we encoded everything that was not an A, T, C or G with a "W"

if (any(grepl("X",PosSequences$sequence))){
  print("Found 'X'")
  PosSequences <- PosSequences[-which(grepl("X",PosSequences$sequence)),]
}
  
if (any(grepl("X",NegSequences$sequence))){
  print("Found 'X'")
  NegSequences <- NegSequences[-which(grepl("X",NegSequences$sequence)),]
}
```

```{r}
# to_onehot:   TGCCAA --> 00010010010010001000    (A = 1000, C = 0100, G = 0010, T = 0001)
PosSequences$sequence <- padding_onehot(PosSequences$sequence)
NegSequences$sequence <- padding_onehot(NegSequences$sequence)

PosSequences_all$sequence <- padding_onehot(PosSequences_all$sequence)
NegSequences_all$sequence <- padding_onehot(NegSequences_all$sequence)

PosSequences$sequence[1]  # The positive sequence, reversed, padded and one-hot encoded
NegSequences$sequence[1]  # The negative sequence, reversed, trimmed and one-hot encoded

# Validation
test1 = all(sapply(list(min(nchar(PosSequences$sequence)),max(nchar(PosSequences$sequence)), mean(nchar(PosSequences$sequence))), function(x) x == 1010))
test2 = all(sapply(list(min(nchar(NegSequences$sequence)),max(nchar(NegSequences$sequence)), mean(nchar(NegSequences$sequence))), function(x) x == 1010))
if (test1 == FALSE) print ("ERROR 1")
if (test2 == FALSE) print ("ERROR 2")
```

```{r}
# Final check: Since some of the sequences might be corrupt, we will delete the ones that are corrupt, if they exist
# We can know if a sequence is corrupt by looking for the W character. When one-hot encoding, we encoded everything that was not an A, T, C or G with a "W"

if (any(grepl("W",PosSequences$sequence))){
  print("Found 'W'")
  PosSequences <- PosSequences[-which(grepl("W",PosSequences$sequence)),]
}
  
if (any(grepl("W",NegSequences$sequence))){
  print("Found 'W'")
  NegSequences <- NegSequences[-which(grepl("W",NegSequences$sequence)),]
}
```

## Training the network

First we have to recover the treated sequences.

```{r}
# Removing duplicates from non-padding sequences
PosUTR <- PosSequences$sequence
PosSequences_nodup <- PosSequences[-which(duplicated(PosUTR)),]
PosUTR <- PosUTR[-which(duplicated(PosUTR))]

NegUTR <- NegSequences$sequence
NegSequences_nodup <- NegSequences[-which(duplicated(NegUTR)),]
NegUTR <- NegUTR[-which(duplicated(NegUTR))]

# Removing duplicates from all the sequences

allPosUTR <- PosSequences_all$sequence
PosSequences_all_nodup <- PosSequences_all[-which(duplicated(allPosUTR)),]

allNegUTR <- NegSequences_all$sequence
NegSequences_all_nodup <- NegSequences_all[-which(duplicated(allNegUTR)),]

# Validation test
test1 = all(NegSequences_nodup$sequence == NegUTR) 
test2 = all(PosSequences_nodup$sequence == PosUTR)
if (test1 == FALSE) print ("ERROR 1")
if (test2 == FALSE) print ("ERROR 2")
```

Now we face an issue with our sequences. They are 808 characters long strings, but R sees them as indivisible elements, not as vectors of 808 elements. We have to transform them before working with them, turning each 808 characters long sequence into 808 sequences of only one character.

```{r}
posNum <- length(PosUTR) # Number of positive sequences
SeqUTR <- append(PosUTR,NegUTR)

old_seq = SeqUTR[1] # For validation tests

n2 <- nchar(SeqUTR[1]) -1 # Number of divisions to make

secuenciasOH <- sapply(1 + 1*0:n2, function(z) substr(SeqUTR, z, z))  # Obtaining the split characters
df <- data.frame(secuenciasOH, "Value" = 0) # Saving them into a dataframe
indx <- sapply(df, is.factor) 
df[indx] <- lapply(df[indx], function(x) as.character(x)) # Factor --> char conversion

df[1:posNum,]$Value <- 1 # We have the value of the positive sequences to positive

table(df$Value)

# Validation tests
test1 = paste(unlist(secuenciasOH[1,]),sep="", collapse="") == old_seq; if (test1 == FALSE) print ("ERROR 1")
test2 = table(df$Value)["0"] == length(NegUTR); if (test2 == FALSE) print ("ERROR 2")
test3 = table(df$Value)["1"] == length(PosUTR); if (test3 == FALSE) print ("ERROR 3")
test4 = all(length(PosUTR) + length(NegUTR) == length(SeqUTR), length(SeqUTR) == nrow(df)); if (test4 == FALSE) print ("ERROR 4")
```

Since we are going to train five different models, we will divide our data into five different dataframes. Each of them will have the same number of positive and negative sequences. Since we don't have many positive sequences, we will train every model with every positive sequences. This way, every model will be trained with the same set of positive sequences, but a different set of negative sequences.

```{r, eval=FALSE}
# Select all the negative sequences
set.seed(1234)
chosenSeqs <- sample(nrow(df) - posNum, size = posNum*5, replace = FALSE, prob = NULL) # Selected rows from 0 to max-posNum
chosenSeqs <- chosenSeqs + posNum

sets <- list()

sets[[1]] <- 1:posNum
sets[[2]] <- (posNum+1):(posNum*2)
sets[[3]] <- ((posNum*2)+1):(posNum*3)
sets[[4]] <- ((posNum*3)+1):(posNum*4)
sets[[5]] <- ((posNum*4)+1):(posNum*5)

```

```{r, eval=FALSE}

df.a <- df[c(1:posNum,chosenSeqs[sets[[1]]]),]
df.b <- df[c(1:posNum,chosenSeqs[sets[[2]]]),]
df.c <- df[c(1:posNum,chosenSeqs[sets[[3]]]),]
df.d <- df[c(1:posNum,chosenSeqs[sets[[4]]]),]
df.e <- df[c(1:posNum,chosenSeqs[sets[[5]]]),]

dfs <- list(df.a, df.b, df.c, df.d, df.e) # 

totalSeq = (nrow(df.a) + nrow(df.b) + nrow(df.c) + nrow(df.d) + nrow(df.e)) 
length(SeqUTR) - totalSeq # Number of sequences that do not appear in the dataframes
test1 = all(sapply(list(table(df.a$Value),table(df.b$Value), table(df.c$Value), table(df.d$Value)), function(x) x == table(df.e$Value)))
test2 = table(df.a$Value)["0"] == posNum
test3 = table(df.a$Value)["1"] == posNum
if (test1) print ("Correct 1") else print ("ERROR 1")
if (test2) print ("Correct 2") else print ("ERROR 2")
if (test3) print ("Correct 3") else print ("ERROR 3")

```

Now we have to divide our data into Training and Test for our model training.

```{r}

output <- c("Value")
trains <- list()
tests <- list()
usedForTrainPos <- list()
usedForTrainNeg <- list()

set.seed(1234)

partitions <- createDataPartition(df[[output]],
                                     p = 0.8,
                                     list = FALSE,
                                     times = 5)

for (i in 1:5){
  
  trains <- append(trains, list(df[partitions[,i],])) # 28210
  tests <- append(tests, list(df[-partitions[,i],])) # 7852
  # Sum of both: 35262

  partPos <- partitions[partitions[,i] <= posNum,i]
  partNeg <- partitions[partitions[,i] > posNum,i]
  partNeg <- partNeg - posNum 
  
  usedForTrainPos <- append(usedForTrainPos,list(unlist(partPos)))
  usedForTrainNeg <- append(usedForTrainNeg,list(unlist(partNeg)))
}

# Validation test

total = 0

for (i in 1:5) {
  total = total + nrow(trains[[i]]) + nrow(tests[[i]])
}

#test1 = total == totalSeq; if (test1 == FALSE) print ("ERROR 1")
test2 = sum(duplicated(usedForTrainPos)) == 0; if (test2 == FALSE) print ("ERROR 2")
test3 = sum(duplicated(usedForTrainNeg)) == 0; if (test3 == FALSE) print ("ERROR 3")

#usedForTrainPos <- usedForTrainPos[-which(duplicated(usedForTrainPos))] # Only the positive genes could be duplicated

```

Now we check the type distribution of the non-3'UTR sequences. We are interested in distinguishing 3'UTR from ICE sequences. So, the desired output would be 0 in every subtype except for ICE.

```{r}
# Checking the type distribution of the train sets

for (i in 1:5){
  negtr <- usedForTrainNeg[[i]]
 # print(min(negtr))
 # print(max(negtr))
  print(table(NegSequences_nodup$detailclass[negtr]))
}


```

Once divided the data, we have to adapt the data to the format the model expects.

```{r}
train.x <- list()
train.y <- list()

test.x <- list()
test.y <- list()

for (i in 1:5){
  train.x <- append(train.x, list(data.matrix(trains[[i]][,1:1010])))
  train.y <- append(train.y, list(data.matrix(trains[[i]][,1011])))
  
  test.x <- append(test.x, list(data.matrix(tests[[i]][,1:1010])))
  test.y <- append(test.y, list(data.matrix(tests[[i]][,1011])))

  train.x[[i]] <- array_reshape(train.x[[i]], c(nrow(trains[[i]]),1010,1))
  test.x[[i]] <- array_reshape(test.x[[i]], c(nrow(tests[[i]]),1010,1))
 }
```

We have to prepare the dataframe that will contain the sequences and the output of the models.

```{r}
# Next step is making a dataframe of every control gene and predicting positive or negative
genesN <- NegSequences_all_nodup
genesP <- PosSequences_all_nodup

 ##### We have to make some transformations to these secuences

n2 <- nchar(genesN$sequence[1]) - 1

secuencesOH <- sapply(1 + 1*0:n2, function(z) substr(genesN$sequence, z, z))  
dfn <- data.frame(secuencesOH, class=genesN$detailclass, id=genesN$id) 
indx <- sapply(dfn, is.factor) 
dfn[indx] <- lapply(dfn[indx], function(x) as.character(x))  #

secuencesOH <- sapply(1 + 1*0:n2, function(z) substr(genesP$sequence, z, z))  
dfp <- data.frame(secuencesOH, class=genesP$detailclass, id=genesP$id) 
indx <- sapply(dfp, is.factor) 
dfp[indx] <- lapply(dfp[indx], function(x) as.character(x))  

# We have got a dataframe with the ensembl_id, sequence and hgnc symbol in genesP and genesN, and a dataframe with the split sequence and hgnc symbol in dfp and dfn

finalSequencesN <- data.matrix(dfn[,1:1010])
finalSequencesN <- array_reshape(finalSequencesN,c(nrow(finalSequencesN),1010,1))
finalSequencesP <- data.matrix(dfp[,1:1010])
finalSequencesP <- array_reshape(finalSequencesP,c(nrow(finalSequencesP),1010,1))

genesP$model1 <- 0
genesP$model2 <- 0
genesP$model3 <- 0
genesP$model4 <- 0
genesP$model5 <- 0

genesN$model1 <- 0
genesN$model2 <- 0
genesN$model3 <- 0
genesN$model4 <- 0
genesN$model5 <- 0

```

Now it is time to build our model. It follows the specifications mentioned in the thesis report. 

```{r}
  batch_size <- 125
  epochs <- 25 
  input_shape <- c(1010,1)
  learn_rate = 0.0001
  
  modelConvolu <- keras_model_sequential()
  modelConvolu %>% 
    layer_conv_1d(filters = 48, kernel_size = 75, activation = "relu", input_shape = input_shape)%>%
    layer_flatten() %>%
    layer_dense(units = 50, activation = 'relu') %>% 
    layer_dense(units = 1, activation="sigmoid") %>%
    
    summary(model)
  
  modelConvolu %>% compile(
    loss = loss_binary_crossentropy,
    optimizer = optimizer_nadam(lr = learn_rate),
    metrics = c('accuracy')
  )
```



```{r, echo=FALSE}
config = tensorflow::tf$ConfigProto(gpu_options = list(allow_growth = TRUE))
sess = tensorflow::tf$Session(config = config)
keras::k_set_session(session = sess)
```

And now we train that model with the data obtained. After each training epoch, we will obtain certain statistics related to the sensitivity and specificity of the model, since they are not metrics that Keras can return from the model.

```{r}
#with(tf$device("/device:GPU:0"), {

models <- list()
all_history <- list()

for (i in 1:5){
  
  sens <- NULL
  spec <- NULL
  history <- NULL
 
  
   modelConvolu <- keras_model_sequential()
  modelConvolu %>% 
    layer_conv_1d(filters = 48, kernel_size = 75, activation = "relu", input_shape = input_shape)%>%
    layer_flatten() %>%
    layer_dense(units = 50, activation = 'relu') %>% 
    layer_dense(units = 1, activation="sigmoid") 
  
  modelConvolu %>% compile(
    loss = loss_binary_crossentropy,
    optimizer = optimizer_nadam(lr = learn_rate),
    metrics = c('accuracy')
  )
  
    historial <- NULL
  
    for (epoch in 1:epochs){
    historial <- modelConvolu %>% fit(
      x = train.x[[i]],
      y = train.y[[i]],
      epochs = 1,  
      batch_size = batch_size, 
      validation_data = list(test.x[[i]], test.y[[i]]),
      verbose = 2)
    
    history <- c(history, historial)
    
    ## Calculating sensitivity and specificity
    pred <- modelConvolu %>% predict(test.x[[i]], batch_size = batch_size)
    prediction.y = round(pred)
    # Confusion matrix
    CM = table(prediction.y, test.y[[i]])
    if (length(CM) > 2){
      sens <- c(sens,sensitivity(CM))
      spec <- c(spec,specificity(CM))
    }
    else {  # This happens in the weird cases the model only predicts positive or negative
      if (sum(grepl(0,prediction.y)) > 0){
        sens <- c(sens, 0)
        spec <- c(spec, 1)
      }
      else{
        sens <- c(sens, 0)
        spec <- c(spec, 1)
      }
    }
  }
  ## Prediction 
  pred <- modelConvolu %>% predict(finalSequencesP, batch_size = batch_size)
  genesP[,5+i] = pred
  
  pred <- modelConvolu %>% predict(finalSequencesN, batch_size = batch_size)
  genesN[,5+i] = pred
  
  models <- append(models, modelConvolu)
  all_history <- append(all_history, historial)
  
  
  
}

#})
```

## Results

Once trained the networks, we will sumarize the results, to later compare them with the other experiment:

```{r}

acc_np_neg <- sum(genesN[,6:10] < 0.5)/(nrow(genesN)*5)
acc_np_pos <- sum(genesP[,6:10] > 0.5)/(nrow(genesP)*5)
acc_np <- (acc_np_neg + acc_np_pos)/2

acc_np_filter_neg <- sum(genesN[nchar(genesN$rawseq) < 202, 6:10] < 0.5)/(sum(nchar(genesN$rawseq) < 202)*5)
acc_np_filter_pos <- sum(genesP[nchar(genesP$rawseq) < 202, 6:10] > 0.5)/(sum(nchar(genesP$rawseq) < 202)*5)
acc_np_filter <- (acc_np_filter_neg + acc_np_filter_pos)/2

error_np_neg <- sum(genesN[,6:10])/(nrow(genesN)*5)
error_np_pos <- sum(1 - genesP[,6:10])/(nrow(genesP)*5)
error_np <- (error_np_neg + error_np_pos)/2

sens_np <- sens
spec_np <- spec

```


# Comparing the results

```{r}

if (acc_p > acc_np){
  print(paste0("The accuracy of the padding experiment is higher than the accuracy of the no-padding experiment by ",acc_p - acc_np))
} else {
  print(paste0("The accuracy of the no-padding experiment is higher than the accuracy of the padding experiment by ",acc_np - acc_p))
}

if (error_p > error_np){
  print(paste0("The error of the padding experiment is higher than the error of the no-padding experiment by ",error_p - error_np))
} else {
  print(paste0("The error of the no-padding experiment is higher than the error of the padding experiment by ",error_np - error_p))
}
```

And lastly, the comparison of the sensitivity and specificity of both experiments:

```{r}

df <- data.frame(sens_p <- sens_p, spec_p <- spec_p, youden_p <- (sens_p + spec_p - 1), sens_np <- sens_np, spec_np <- spec_np, youden_np <- (sens_np + spec_np - 1))

ggplot(df, aes(x = 1:(nrow(df)))) + 
  geom_line(aes(y = sens_p, colour = "Sensitivity of Padding sequences")) + 
  geom_line(aes(y = sens_np, colour = "Sensitivity of non-Padding sequences")) +
  labs(x = "Training epoch", y = "Value of Sensitivity", legend="Colour", title="Comparison of Sensitivity")

ggplot(df, aes(x = 1:(nrow(df)))) + 
  geom_line(aes(y = spec_p, colour = "Specificity of Padding sequences")) + 
  geom_line(aes(y = spec_np, colour = "Specificity of non-Padding sequences")) +
  labs(x = "Training epoch", y = "Value of Specificity", legend="Colour", title="Comparison of Specificity") 

ggplot(df, aes(x = 1:(nrow(df)))) + 
  geom_line(aes(y = youden_p, colour = "Youden's index of Padding sequences")) + 
  geom_line(aes(y = youden_np, colour = "Youden's index of non-Padding sequences")) +
  labs(x = "Training epoch", y = "Value of Youden's index", legend="Colour", title="Comparison of Youden's index")


```

While the padding version has higher sensitivity at the end of the training, the non-padding version has higher sensitivity and specificity. That, and the fact that the non-padding version has higher accuracy and smaller error, makes it the better version of the tho, although just by a small margin.