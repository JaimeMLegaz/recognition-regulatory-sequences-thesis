---
title: "Training the network of the UTR experiment"
author: "Jaime MartÃ­nez Legaz"
output:
  html_document:
    df_print: paged
---

## Loading the libraries

Loading all required libraries

```{r, eval=FALSE, echo=FALSE}
library(tensorflow)

with(tf$device("/gpu:0"), {

})

```

```{r}
library(biomaRt)
library(SetupSequences)
library(keras)
library(caret)
```

## Obtaining the sequences

We have to obtain the sequences we will work with. This is the code that will achieve that.

```{r}
library(tidyverse)
library(BSgenome)
library(BSgenome.Hsapiens.UCSC.hg38.masked)
library(GenomicRanges)


ml_table = readRDS(file = "../Data/ml_data_rf.rds") %>% 
  dplyr::select(class) %>% 
  rownames_to_column("id") %>% 
  tidyr::separate(id, c("gene", "seqnames", "start", "end", "strand", "tmp"), sep=":", remove=FALSE)
  

# converting to Granges object
ml_table_gr = makeGRangesFromDataFrame(ml_table, keep.extra.columns = TRUE)

# adding "chr" in front of seqnames
newStyle <- mapSeqlevels(seqlevels(ml_table_gr), "UCSC")
ml_table_gr <- renameSeqlevels(ml_table_gr, newStyle)

# Extract sequences using the package BSgenome
data = data.frame(
  id = ml_table$id,
  class = ml_table$class,
  sequence = BSgenome::getSeq(Hsapiens, ml_table_gr, as.character = TRUE) #as.character=TRUE does not work?
)
data$sequence <- as.character(data$sequence)
table(data$class)
```

Next step is grouping all the non 3'UTR sequences into a group, so that we can compare 3'UTR sequences with non 3'UTR sequences regardless of the subtype.

```{r}
old_data <- data # Needed for the valdation tests

data <- data[data$class == "ICE" | data$class == "UTR",]
data$detailclass = data$class
data$class = ifelse(data$class %in% "UTR", "UTR", "Non_3_UTR")  %>% as.factor() %>% as.integer()
data$class <- data$class - 1 #0 = No3UTR 1 = 3UTR
table(data$class)
```

```{r}
# Validation test
test1 = table(data$class)["0"] == nrow(data) - table(old_data$class)["UTR"]; if (test1 == FALSE) print ("ERROR 1")
test2 = table(data$class)["1"] == table(old_data$class)["UTR"]; if (test2 == FALSE) print ("ERROR 2")

```

We will separate these sequences into the "Positive" group (3'UTR) and the "Negative" group (non-3'UTR).

```{r}
PositiveSequences = data[data$class == 1,]
NegativeSequences = data[data$class == 0,]
```

And save these sequences to a file.

```{r}
write.csv(PositiveSequences,file="../Data/PositiveUTRSequences_untreated_sid", row.names = FALSE, quote = FALSE)
write.csv(NegativeSequences,file="../Data/NegativeUTRSequences_untreated_sid", row.names = FALSE, quote = FALSE)
```


## Treating the sequences

First, we recover the sequences we obtained with ensembl.

```{r}
PosSequences <- PositiveSequences
NegSequences <- NegativeSequences
```

Next we start with the treatment of sequences. We have to:

- Reverse them
- Make sure all sequences are 250 characters long
- One-hot encode them

For that, we need some functions from the custom package developed for this project.

```{r}
# We save the sequences for later use
PosSequences$rawseq <- PosSequences$sequence
NegSequences$rawseq <- NegSequences$sequence

# Example of sequence: AACCGT
PosSequences$sequence[1]  # Example of a positive sequence
NegSequences$sequence[1]  # Example of a negative sequence
```


```{r}
old_seq = PosSequences$sequence[1]

# strReverse:  AACCGT --> TGCCAA
PosSequences$sequence <- strReverse(PosSequences$sequence)
NegSequences$sequence <- strReverse(NegSequences$sequence)

PosSequences$sequence[1]  # The positive sequence, once reversed
NegSequences$sequence[1]  # The negative sequence, once reversed

# Validation
doublereverse = strReverse(PosSequences$sequence[1])
test1 = doublereverse == old_seq; if (test1 == FALSE) print ("ERROR 1")
```

```{r}
# padding_sequences: Inserts padding characters ("X") in sequences shorter than 250 characters, and trims sequences longer than 250 characters
PosSequences$sequence <- padding_sequences(PosSequences$sequence)
NegSequences$sequence <- padding_sequences(NegSequences$sequence)

PosSequences$sequence[1]  # The positive sequence, with some characters added so it can be 250 characters long
NegSequences$sequence[1]  # The negative sequence, trimmed so it becomes 250 characters long

# Validation
test1 = all(sapply(list(min(nchar(PosSequences$sequence)),max(nchar(PosSequences$sequence)), mean(nchar(PosSequences$sequence))), function(x) x == 250))
test2 = all(sapply(list(min(nchar(NegSequences$sequence)),max(nchar(NegSequences$sequence)), mean(nchar(NegSequences$sequence))), function(x) x == 250))
if (test1 == FALSE) print ("ERROR 1")
if (test2 == FALSE) print ("ERROR 2")
```

```{r}
# to_onehot:   TGCCAA --> 00010010010010001000    (A = 1000, C = 0100, G = 0010, T = 0001)
PosSequences$sequence <- to_onehot(PosSequences$sequence)
NegSequences$sequence <- to_onehot(NegSequences$sequence)

PosSequences$sequence[1]  # The positive sequence, reversed, padded and one-hot encoded
NegSequences$sequence[1]  # The negative sequence, reversed, trimmed and one-hot encoded

# Validation
test1 = all(sapply(list(min(nchar(PosSequences$sequence)),max(nchar(PosSequences$sequence)), mean(nchar(PosSequences$sequence))), function(x) x == 1250))
test2 = all(sapply(list(min(nchar(NegSequences$sequence)),max(nchar(NegSequences$sequence)), mean(nchar(NegSequences$sequence))), function(x) x == 1250))
if (test1 == FALSE) print ("ERROR 1")
if (test2 == FALSE) print ("ERROR 2")
```

```{r}
# Final check: Since some of the sequences might be corrupt, we will delete the ones that are corrupt, if they exist
# We can know if a sequence is corrupt by looking for the W character. When one-hot encoding, we encoded everything that was not an A, T, C or G with a "W"

if (any(grepl("W",PosSequences$sequence))){
  print("Found 'W'")
  PosSequences <- PosSequences[-which(grepl("W",PosSequences$sequence)),]
}
  
if (any(grepl("W",NegSequences$sequence))){
  print("Found 'W'")
  NegSequences <- NegSequences[-which(grepl("W",NegSequences$sequence)),]
}
```

Once treated, we can save them in a file for later use.

```{r}
fileCon<-file("Pos3UTR")
write.table(PosSequences$sequence,file = fileCon, quote=FALSE, row.names = FALSE, col.names = FALSE)
fileCon<-file("Neg3UTR")
write.table(NegSequences$sequence,file = fileCon, quote=FALSE, row.names = FALSE, col.names = FALSE)
```

## Training the network

First we have to recover the treated sequences.

```{r}
PosUTR <- scan(file="../Data/PosUTR",what="character")
length(PosUTR)
PosSequences_nodup <- PosSequences[-which(duplicated(PosUTR)),]
PosUTR <- PosUTR[-which(duplicated(PosUTR))]
length(PosUTR)

NegUTR <- scan(file="../Data/NegUTR",what="character")
length(NegUTR)
NegSequences_nodup <- NegSequences[-which(duplicated(NegUTR)),]
NegUTR <- NegUTR[-which(duplicated(NegUTR))]
length(NegUTR)

# Validation test
test1 = all(NegSequences_nodup$sequence == NegUTR) 
test2 = all(PosSequences_nodup$sequence == PosUTR)
if (test1 == FALSE) print ("ERROR 1")
if (test2 == FALSE) print ("ERROR 2")
```

Now we face an issue with our sequences. They are 1250 characters long strings, but R sees them as indivisible elements, not as vectors of 1250 elements. We have to transform them before working with them, turning each 1250 characters long sequence into 1250 sequences of only one character.

```{r}
posNum <- length(PosUTR) # Number of positive sequences
SeqUTR <- append(PosUTR,NegUTR)

old_seq = SeqUTR[1] # For validation tests

n2 <- nchar(SeqUTR[1]) -1 # Number of divisions to make

secuenciasOH <- sapply(1 + 1*0:n2, function(z) substr(SeqUTR, z, z))  # Obtaining the split characters
df <- data.frame(secuenciasOH, "Value" = 0) # Saving them into a dataframe
indx <- sapply(df, is.factor) 
df[indx] <- lapply(df[indx], function(x) as.character(x)) # Factor --> char conversion

df[1:posNum,]$Value <- 1 # We have the value of the positive sequences to positive

table(df$Value)

# Validation tests
test1 = paste(unlist(secuenciasOH[1,]),sep="", collapse="") == old_seq; if (test1 == FALSE) print ("ERROR 1")
test2 = table(df$Value)["0"] == length(NegUTR); if (test2 == FALSE) print ("ERROR 2")
test3 = table(df$Value)["1"] == length(PosUTR); if (test3 == FALSE) print ("ERROR 3")
test4 = all(length(PosUTR) + length(NegUTR) == length(SeqUTR), length(SeqUTR) == nrow(df)); if (test4 == FALSE) print ("ERROR 4")
```

Since we are going to train five different models, we will divide our data into five different dataframes. Each of them will have the same number of positive and negative sequences. Since we don't have many positive sequences, we will train every model with every positive sequences. This way, every model will be trained with the same set of positive sequences, but a different set of negative sequences.

```{r}
# Select all the negative sequences
set.seed(1234)
chosenSeqs <- sample(nrow(df) - posNum, size = posNum*5, replace = FALSE, prob = NULL) # Selected rows from 0 to max-posNum
chosenSeqs <- chosenSeqs + posNum

sets <- list()

sets[[1]] <- 1:posNum
sets[[2]] <- (posNum+1):(posNum*2)
sets[[3]] <- ((posNum*2)+1):(posNum*3)
sets[[4]] <- ((posNum*3)+1):(posNum*4)
sets[[5]] <- ((posNum*4)+1):(posNum*5)

```

```{r}

df.a <- df[c(1:posNum,chosenSeqs[sets[[1]]]),]
df.b <- df[c(1:posNum,chosenSeqs[sets[[2]]]),]
df.c <- df[c(1:posNum,chosenSeqs[sets[[3]]]),]
df.d <- df[c(1:posNum,chosenSeqs[sets[[4]]]),]
df.e <- df[c(1:posNum,chosenSeqs[sets[[5]]]),]

dfs <- list(df.a, df.b, df.c, df.d, df.e) # 

totalSeq = (nrow(df.a) + nrow(df.b) + nrow(df.c) + nrow(df.d) + nrow(df.e)) 
length(SeqUTR) - totalSeq # Number of sequences that do not appear in the dataframes
test1 = all(sapply(list(table(df.a$Value),table(df.b$Value), table(df.c$Value), table(df.d$Value)), function(x) x == table(df.e$Value)))
test2 = table(df.a$Value)["0"] == posNum
test3 = table(df.a$Value)["1"] == posNum
if (test1) print ("Correct 1") else print ("ERROR 1")
if (test2) print ("Correct 2") else print ("ERROR 2")
if (test3) print ("Correct 3") else print ("ERROR 3")

```

Now we have to divide our data into Training and Test for our model training.

```{r}

output <- c("Value")
trains <- list()
tests <- list()
usedForTrainPos <- list()
usedForTrainNeg <- list()

set.seed(1234)

for (i in 1:5){
  partition <- createDataPartition(dfs[[i]][[output]],
                                     p = 0.8,
                                     list = FALSE,
                                     times = 1)
  trains <- append(trains, list(dfs[[i]][partition,])) # 28210
  tests <- append(tests, list(dfs[[i]][-partition,])) # 7852
  # Sum of both: 35262

  partPos <- partition[partition <= posNum]
  partNeg <- partition[partition > posNum]
  partNeg <- partNeg - posNum
  
  usedForTrainPos <- append(usedForTrainPos,list(unlist(partPos)))
  usedForTrainNeg <- append(usedForTrainNeg,list(unlist(chosenSeqs[sets[[i]][partNeg]])))
}

# Validation test

total = 0

for (i in 1:5) {
  total = total + nrow(trains[[i]]) + nrow(tests[[i]])
}

test1 = total == totalSeq; if (test1 == FALSE) print ("ERROR 1")
test2 = sum(duplicated(usedForTrainPos)) == 0; if (test2 == FALSE) print ("ERROR 2")
test3 = sum(duplicated(usedForTrainNeg)) == 0; if (test3 == FALSE) print ("ERROR 3")

#usedForTrainPos <- usedForTrainPos[-which(duplicated(usedForTrainPos))] # Only the positive genes could be duplicated

```

Now we check the type distribution of the non-3'UTR sequences. We are interested in distinguishing 3'UTR from ICE sequences. So, the desired output would be 0 in every subtype except for ICE.

```{r}
# Checking the type distribution of the train sets

for (i in 1:5){
  negtr <- usedForTrainNeg[[i]]
 # print(min(negtr))
 # print(max(negtr))
  print(table(NegSequences_nodup$detailclass[negtr]))
}


```

Once divided the data, we have to adapt the data to the format the model expects.

```{r}
train.x <- list()
train.y <- list()

test.x <- list()
test.y <- list()

for (i in 1:5){
  train.x <- append(train.x, list(data.matrix(trains[[i]][,1:1250])))
  train.y <- append(train.y, list(data.matrix(trains[[i]][,1251])))
  
  test.x <- append(test.x, list(data.matrix(tests[[i]][,1:1250])))
  test.y <- append(test.y, list(data.matrix(tests[[i]][,1251])))

  train.x[[i]] <- array_reshape(train.x[[i]], c(nrow(trains[[i]]),1250,1))
  test.x[[i]] <- array_reshape(test.x[[i]], c(nrow(tests[[i]]),1250,1))
 }
```

We have to prepare the dataframe that will contain the sequences and the output of the models.

```{r}
# Next step is making a dataframe of every control gene and predicting positive or negative
genesN <- NegSequences_nodup
genesP <- PosSequences_nodup

 ##### We have to make some transformations to these secuences

n2 <- nchar(genesN$sequence[1]) - 1

secuencesOH <- sapply(1 + 1*0:n2, function(z) substr(genesN$sequence, z, z))  
dfn <- data.frame(secuencesOH, class=genesN$detailclass, id=genesN$id) 
indx <- sapply(dfn, is.factor) 
dfn[indx] <- lapply(dfn[indx], function(x) as.character(x))  #

secuencesOH <- sapply(1 + 1*0:n2, function(z) substr(genesP$sequence, z, z))  
dfp <- data.frame(secuencesOH, class=genesP$detailclass, id=genesP$id) 
indx <- sapply(dfp, is.factor) 
dfp[indx] <- lapply(dfp[indx], function(x) as.character(x))  

# We have got a dataframe with the ensembl_id, sequence and hgnc symbol in genesP and genesN, and a dataframe with the split sequence and hgnc symbol in dfp and dfn

finalSequencesN <- data.matrix(dfn[,1:1250])
finalSequencesN <- array_reshape(finalSequencesN,c(nrow(finalSequencesN),1250,1))
finalSequencesP <- data.matrix(dfp[,1:1250])
finalSequencesP <- array_reshape(finalSequencesP,c(nrow(finalSequencesP),1250,1))

genesP$model1 <- 0
genesP$model2 <- 0
genesP$model3 <- 0
genesP$model4 <- 0
genesP$model5 <- 0

genesN$model1 <- 0
genesN$model2 <- 0
genesN$model3 <- 0
genesN$model4 <- 0
genesN$model5 <- 0

```

Now it is time to build our model. It follows the specifications mentioned in the thesis report. 

```{r}
  batch_size <- 125
  epochs <- 25 
  input_shape <- c(1250,1)
  learn_rate = 0.0001
  
  modelConvolu <- keras_model_sequential()
  modelConvolu %>% 
    layer_conv_1d(filters = 48, kernel_size = 75, activation = "relu", input_shape = input_shape)%>%
    layer_flatten() %>%
    layer_dense(units = 50, activation = 'relu') %>% 
    layer_dense(units = 1, activation="sigmoid") %>%
    
    summary(model)
  
  modelConvolu %>% compile(
    loss = loss_binary_crossentropy,
    optimizer = optimizer_nadam(lr = learn_rate),
    metrics = c('accuracy')
  )
```



```{r, echo=FALSE}
config = tensorflow::tf$ConfigProto(gpu_options = list(allow_growth = TRUE))
sess = tensorflow::tf$Session(config = config)
keras::k_set_session(session = sess)
```

And now we train that model with the data obtained. After each training epoch, we will obtain certain statistics related to the sensitivity and specificity of the model, since they are not metrics that Keras can return from the model.

```{r}
#with(tf$device("/device:GPU:0"), {

models <- list()

for (i in 1:5){
  
  sens <- NULL
  spec <- NULL
  history <- NULL
 
  
   modelConvolu <- keras_model_sequential()
  modelConvolu %>% 
    layer_conv_1d(filters = 48, kernel_size = 75, activation = "relu", input_shape = input_shape)%>%
    layer_flatten() %>%
    layer_dense(units = 50, activation = 'relu') %>% 
    layer_dense(units = 1, activation="sigmoid") 
  
  modelConvolu %>% compile(
    loss = loss_binary_crossentropy,
    optimizer = optimizer_nadam(lr = learn_rate),
    metrics = c('accuracy')
  )
  
    for (epoch in 1:epochs){
    historial <- modelConvolu %>% fit(
      x = train.x[[i]],
      y = train.y[[i]],
      epochs = 1,  
      batch_size = batch_size, 
      validation_data = list(test.x[[i]], test.y[[i]]),
      verbose = 2)
    
    history <- c(history, historial)
  }
  ## Prediction 
  pred <- modelConvolu %>% predict(finalSequencesP, batch_size = batch_size)
  genesP[,5+i] = pred
  
  pred <- modelConvolu %>% predict(finalSequencesN, batch_size = batch_size)
  genesN[,5+i] = pred
  
  models <- append(models, modelConvolu)
}

#})
```

## Determining the relevances of the characters

We will obtain the relevances of the characters of the sequences using the LRP algorithm. An in-depth explanation of said algorithm can be found in the thesis report.


```{r}
lrp <- function(model, input){ # Asks for the model and the input sequence
 # print(paste0("Sequence number ",indexSeq)) # Shows the number of the sequence
  layers <- model$layers # Gets the layers of the model
  
  ## STEP 1: CREATING DATA STRUCTURES
  
  activs <- list() # For the activations of the neurons
  weights <- list() # For the weights of the connections

  for (i in 1:length(layers)){ # For each layer, we will save all the activations of its neurons, and weights of its connections
    lay <- layers[[i]]
    layer_name <- lay$name
    
    intermediate_layer_model <- keras_model(inputs = modelConvolu$input,  
                                      outputs = get_layer(modelConvolu, layer_name)$output) # Creates an incomplete model up to the selected layer
    intermediate_output <- predict(intermediate_layer_model, array_reshape(input, c(1,1250,1))) # Gets the output of that layer for the selected sequence
    
    if (length(lay$weights) > 0){ # If not a flatten layer (those do not have weights)
      activs[[i]] <- intermediate_output # Saves the activations
      weights[[i]] <- lay$get_weights()[[1]] # Saves the weights
    }
  }
  
  ## STEP 2: OUTPUT LAYER
  
  Rk <- list() # Rk will contain the relevances of the layers
  Rk[["output"]] <- as.numeric(activs[[length(layers)]])  # For the output layer, the relevance is equal to the activation of the output node
  if (Rk[["output"]] < 0.5) # If the output is lesser than 0.5 it means it is a negative sequence, and we have to compute the relevance towards the negative class
    Rk[["output"]] <- 1 - Rk[["output"]]
  
  ## STEP 3: DENSE LAYER
  
  # First of all we have to turn all negative weights into zeroes, since we only use the positive weights with this formula
  pos_weights <- weights[[4]]
  for (i in 1:length(weights[[4]]))
    pos_weights[i] <- max(weights[[4]][i],0) # Not using the apply family functions so it does not alter the shape
  
  denom = 0 # Denominator of the formula
  
  for (i in 1:length(activs[[3]])) { # Calculating the denominator
    value <- activs[[3]][i] * pos_weights[i]
    denom <- denom + value
  }
  
  for (i in 1:length(activs[[3]])) { # Calculating each numerator and, with it, the relevance of each neuron of the dense layer
    numer <- activs[[3]][i] * pos_weights[i]
    if (denom != 0) { # We do not want to divide by zero
        division <- numer/denom
      } else {
        division <- 0
      }
    Rk[["dense"]][i] <- division * Rk[["output"]][1] 
  }
  
  ## STEP 4: CONVOLUTIONAL LAYER
  
  # Same as before, no negative weights
  pos_weights <- weights[[3]]
  for (i in 1:length(weights[[3]]))
    pos_weights[i] <- max(weights[[3]][i],0) # Not using the apply family functions so it does not alter the shape
  
  for (i in 1:length(activs[[1]])) # Creating the data structure for the relevances, and setting its values to zero
    Rk[["conv"]][i] = 0
  
  for (k in 1:length(Rk[["dense"]])) { # For each neuron in the layer analyzed before we have to repeat the main cycle
    denom <- 0
    
    for (j in 1:length(activs[[1]])) { # For each neuron in the actual layer, calculate denominator
      value <- activs[[1]][j] * pos_weights[j,k]
      denom <- denom + value
    }
    
    for (j in 1:length(activs[[1]])) {# For each neuron in the actual layer, calculate numerator and relevance j<-k
      numer <- activs[[1]][j] * pos_weights[j,k]
      if (denom != 0) {
        division <- numer/denom
      } else {
        division <- 0
      }
      Rk[["conv"]][j] <- Rk[["conv"]][j] + (division * Rk[["dense"]][k])
    }
  }
  
  ## STEP 5: INPUT LAYER
  
  # Like before, only positive weights
  pos_weights <- weights[[1]]
  for (i in 1:length(weights[[1]]))
    pos_weights[i] <- max(weights[[1]][i],0) # Not using the apply family functions so it does not alter the shape
  
  used <- list() # We want to normalize the relevances, and not every input value is used the same number of times. This list contains the number of times an input value is used
  for (i in 1:1250){ # Creating the data structures for the Relevance and the times used, and setting the values to zero
    Rk[["input"]][i] = 0
    used[["input"]][i] = 0
  }
  
  for (k in 1:length(activs[[1]])){ # For each one of the convolution cells
    ## First we have to obtain the list of input nodes that contribute to this value. 
    ## Before that, we have to get the number of neurons that contributed. We know that there are as many nodes as the size of the filters:
    num_neurons <- length(weights[[1]][,,1])
    
    ## Now we have to know which are these neurons. For that reason, we have to obtain the position of this cell inside its convolution filter
    # Since a%%b returns a value in [0,b-1] and we want a value in [1,b], we have to do it this way:
    cells_per_kernel <- length(activs[[1]][,,1]) # Number of cells in a kernel
    k_relative <- ((k-1)%%cells_per_kernel)+1 # Position of the cell inside its kernel
    # And now we obtain the list of input values that affected this particular convolution cell
    j_neurons <- k_relative:(k_relative + num_neurons - 1) # Neurons that affected the actual convolution cell
    
    ## To obtain the weights, we have to know to which kernel this cell belongs to
    kernel <- ceiling(k/cells_per_kernel)
    
    ## And now we can obtain said weights
    kernel_weights <- pos_weights[,,kernel]
    
    ## Now we can calculate the denominator for this filter, following the formula
    denom <- sum(input[j_neurons] * kernel_weights)
    
    ## And, for each j neuron, we calculate its numerator, and add its value to its relevance
    for (j in 1:length(j_neurons)){
      numer <- input[j_neurons[j]] * kernel_weights[j] * Rk[["conv"]][k]
      if (denom != 0) {
        division <- numer/denom
      } else {
        division <- 0
      }
      Rk[["input"]][j_neurons[j]] = Rk[["input"]][j_neurons[j]] + division
      used[["input"]][j_neurons[j]] = used[["input"]][j_neurons[j]] + 1
    }
  }
  
  ## STEP 6: RETURNING RESULT
  
  return(Rk[["input"]] / used[["input"]])
    
}
```

We now calculate the relevances for some sequences, and save them in a file. We separate positive from negative sequences since they will have different relevances.

```{r}
relevancesP <- genesP
relevancesN <- genesN

posS <- df[df$Value == 1,] # Positive sequences
negS <- df[df$Value == 0,] # Negative sequences

for (i in 1:250){ # We create the empty list of relevances, that will later contain the relevances for the positions of the sequences
  relevancesP[[paste0("relevance",i)]] <- 0 # Positive relevances
  relevancesN[[paste0("relevance",i)]] <- 0 # Negative relevances
}

for (i in 1:10){ # For each sequence we want to calculate its relevance:
  relevances <- lrp(modelConvolu, as.matrix(as.numeric(posS[i,1:1250]))) # Apply the LRP algorithm and obtain the relevance of each one-hot digit
  
  for (j in 1:250){ # Since we want the relevances of the characters (250), not the one-hot digits (1250), we have to combine them
    k = 1 + (5*(j - 1)) # j is the character, and k:(k+4) are its 5 one-hot digits
    relevancesP[i,paste0("relevance", j)] <- relevances[k] + relevances[k + 1] + relevances[k + 2] + relevances[k + 3] + relevances[k + 4]
  }
  
#  write.csv(relevancesP[1:i,],"../Results/posRelevances", row.names = FALSE)
}

for (i in 1:10){ # We do the same for the negative sequences
  relevances <- lrp(modelConvolu, as.matrix(as.numeric(negS[i,1:1250])))
  
  for (j in 1:250){
    k = 1 + (5*(j - 1))
    relevancesN[i,paste0("relevance", j)] <- relevances[k] + relevances[k + 1] + relevances[k + 2] + relevances[k + 3] + relevances[k + 4]
  }
  
#  write.csv(relevancesN[1:i,],"../Results/negRelevances", row.names = FALSE)
}


```

## Visualization of the results

In this section we will visualize the relevances obtained from the LRP algorithm. We will provide multiple plots, showing relevances of positive and negative sequences, both individually (each sequence being a line inside the plot) and collectively (only one line in the plot, beign that line the mean of the sequences).

First of all, we have to define a new function that will help us normalize our relevances so that we can compare the positive and negative sequences comfortably, since at the moment we are working with very small values. A more detailed explanation of relevances and how they work is explained in the thesis report.

One important thing that we have to take into account is that not all sequences have the same combined relevance. The combined relevance of a sequence, even before normalization, varies from 0 to 1, depending on the output of the network for that combination of sequence and class. So, a sequence classified as 98% positive will have a total relevance of 0.98, but a sequence classified as 54% positive will have a total relevance of 0.54. That explains why some sequence lines can appear above or below in the graph.

```{r}

normalizeRelList <- function(listp, listn){
  all_mx <- 0
  all_mn <- 99999999
  
  for (i in 1:length(listp)){
    mx <- max(listp[[i]])
    mn <- min(listp[[i]])
    
    if (mx > all_mx) all_mx = mx
    if (mn < all_mn) all_mn = mn
  }
  
  for (i in 1:length(listn)){
    mx <- max(listn[[i]])
    mn <- min(listn[[i]])
    
    if (mx > all_mx) all_mx = mx
    if (mn < all_mn) all_mn = mn
  }
  
  for (i in 1:(length(listp))){ # Normalize using those highest and lowest values
    listp[[i]] <- (listp[[i]] - all_mn) / (all_mx - all_mn)
  }
  
  for (i in 1:(length(listn))){ # Normalize using those highest and lowest values
    listn[[i]] <- (listn[[i]] - all_mn) / (all_mx - all_mn)
  }
  
  return(list(listp,listn))
}

#normalizeRelList <- function(list){
#  len_list <- length(list) # Get the number of models we are working with (each model has a set of relevances)
  
 # all_mx <- 0
#  all_mn <- 9999999
  
 # for (i in 1:len_list){ # Obtain the highest and lowest value of relevance
 #   mx <- max(list[[i]])
 #   mn <- min(list[[i]])
    
 #   if (mx > all_mx) all_max = mx
#    if (mn < all_mn) all_mn = mn
 # }
  
#  list
  
 # for (i in 1:len_list){ # Normalize using those highest and lowest values
 #   list[[i]] <- (list[[i]] - mn) / (mx - mn)
 # }
 
#  return(list) 
#}

normalizeDF <- function(posdf, negdf){
 mn <- min(posdf, negdf)
 mx <- max(posdf, negdf)
 
 posd <- (posdf - mn) / (mx - mn)
 negd <- (negdf - mn) / (mx - mn)
 
  return(list(posd, negd))
}


```

Once defined the function, we load all the relevances we calculated previously. 

We have to remember we used cross-validation to train the models, and, for that reason, we have 5 different models right now. Each model will have different relevances since they were trained with different sequences. 

```{r}

pos_rel <- list()
for (i in 1:5)  # Loading the positive relevances
{
  rel_file = paste0("../Results/Relevances/posRelevancesModel",i)
  pos_rel <- append(pos_rel, list(read.csv(rel_file, sep=",", stringsAsFactors = FALSE)[,11:260]))
}

neg_rel <- list()
for (i in 1:5) # Loading the negative relevances
{
  rel_file = paste0("../Results/Relevances/negRelevancesModel",i)
  neg_rel <- append(neg_rel, list(read.csv(rel_file, sep=",", stringsAsFactors = FALSE)[,11:260]))
}

# Obtain the mean of the 5 models
# mean_pos_rel <- colMeans(rbind(pos_rel[[1]], pos_rel[[2]], pos_rel[[3]], pos_rel[[4]], pos_rel[[5]]))
# mean_neg_rel <- colMeans(rbind(neg_rel[[1]], neg_rel[[2]], neg_rel[[3]], neg_rel[[4]], neg_rel[[5]]))

# p_rel <- pr[,11:260] 
# n_rel <- nr[,11:260] 

# p_rel_sum <- colSums(pr[,11:260])      
# n_rel_sum <- colSums(nr[,11:260])

```

Now that we have the relevances loaded, we will start with the data visualization. In all the plots we will show now, the x-axis will correspond to the position of the character, and the y-axis will be its relevance. For example, a point of X = 100 and Y = 0.3 means that the 100th character in the sequence had a normalized relevance of 0.3.

The first five plots we will look at contains all the sequences of one of the five models in a single graph. We will paint the positive sequences in red, and the negative sequences in blue. Also, a dotted line represents the padding characters in a sequence, while a continuous line represents the useful characters (A,C,G,T). Later we will study the relevance of these characters more in depth.


```{r}

norms <- normalizeRelList(pos_rel, neg_rel)
pos_norm <- norms[[1]]
neg_norm <- norms[[2]]

for (i in 1:5){ # One plot for every model
  pr_norm <- pos_norm[[i]] # The positive and negative relevances of the current model
  nr_norm <- neg_norm[[i]]
  
  plot(type="n", xlim <- c(1,250), c(min(pr_norm,nr_norm), max(pr_norm,nr_norm)), xlab = "Position of the character in the sequence", ylab="Relevance of the character")
  for (j in 1:nrow(pr_norm)){
    numX <- max(0, 250 - nchar(relevancesP$rawseq[j]))
    
    lines(colSums(pr_norm[j,1:(250-numX)]), col="blue") # In blue -> Relevances of CAGT
  }
  for (j in 1:nrow(pr_norm)){
    numX <- max(0, 250 - nchar(relevancesP$rawseq[j]))
    if(numX > 0)
      lines(x = (250-numX):250, y = colSums(pr_norm[j,(250-numX):250]), col="blue", lty=2) # In red -> Relevances of X
  }
  
  for (j in 1:nrow(nr_norm)){
    numX <- max(0, 250 - nchar(relevancesN$rawseq[j]))
    
    lines(colSums(nr_norm[j,1:(250-numX)]), col="red") # In blue -> Relevances of CAGT
  }
  for (j in 1:nrow(pr_norm)){
    numX <- max(0, 250 - nchar(relevancesN$rawseq[j]))
    if(numX > 0)
      lines(x = (250-numX):250, y = colSums(nr_norm[j,(250-numX):250]), col="red", lty=2) # In red -> Relevances of X
  }
  
}


```

Looking at the plots above, it is obvious that the relevances are the same regardless of the model used. For that reason, we will calculate the relevances from now on using only one model to save some time.

There are a couple things that we would like to point out:

- The red lines tend to be higher than the blue lines at the beginning of the sequence, and lower at the ending
- The dotted lines tend to be in sequences with lower relevance
- The relevances follow a similar distribution in both types of sequence, except at the very end

These are interesting facts, but we need more than 20 sequences to study these cases, since they could be isolated ocurrences. We will load a second set of relevances, with 500 sequences instead of 20.

```{r}
pos_rel <- read.csv("../Results/Relevances/posRelevances", sep=",", stringsAsFactors = FALSE)[,11:260]
neg_rel <- read.csv("../Results/Relevances/negRelevances", sep=",", stringsAsFactors = FALSE)[,11:260]

# Updating the dataframe that asigns relevances to sequences
relevancesP[1:nrow(pos_rel),11:260] <- pos_rel
relevancesN[1:nrow(neg_rel),11:260] <- neg_rel
```

```{r}
norm <- normalizeDF(pos_rel, neg_rel)
pos_norm <- norm[[1]]
neg_norm <- norm[[2]]

plot(type="n", xlim <- c(1,250), c(min(pos_norm, neg_norm), max(pos_norm, neg_norm)), xlab = "Position of the character in the sequence", ylab="Relevance of the character")

  
  for (j in 1:nrow(pos_norm)){
    numX <- max(0, 250 - nchar(relevancesP$rawseq[j]))
    
    lines(colSums(pos_norm[j,1:(250-numX)]), col="blue") # In blue -> Relevances of CAGT
  }
  for (j in 1:nrow(pos_norm)){
    numX <- max(0, 250 - nchar(relevancesP$rawseq[j]))
    if(numX > 0)
      lines(x = (250-numX):250, y = colSums(pos_norm[j,(250-numX):250]), col="blue", lty=2) # In red -> Relevances of X
  }
  
  for (j in 1:nrow(neg_norm)){
    numX <- max(0, 250 - nchar(relevancesN$rawseq[j]))
    
    lines(colSums(neg_norm[j,1:(250-numX)]), col="red") # In blue -> Relevances of CAGT
  }
  for (j in 1:nrow(neg_norm)){
    numX <- max(0, 250 - nchar(relevancesN$rawseq[j]))
    if(numX > 0)
      lines(x = (250-numX):250, y = colSums(neg_norm[j,(250-numX):250]), col="red", lty=2) # In red -> Relevances of X
  }

```

We can see that this last plot is very difficult to interpret, due to the high density of lines. For that reason, we have to think of an alternative representation. 

Instead of showing every line in the plot, we will work with the means of the sequence. This way, we will have only two lines: One for the positive sequences, and one for the negative.

```{r}

mean_pos_norm <- colSums(pos_norm)/nrow(pos_norm)
mean_neg_norm <- colSums(neg_norm)/nrow(neg_norm)

# TODO: AÃ±adir leyenda

plot(type="n", xlim <- c(1,250), c(min(mean_pos_norm, mean_neg_norm), max(mean_pos_norm, mean_neg_norm)), xlab = "Position of the character in the sequence", ylab="Mean relevance of the character")
lines(mean_pos_norm, col="blue")
lines(mean_neg_norm, col="red")
legend("topright", legend=c("Positive", "Negative"),
       col=c("blue", "red"), lty=1, cex=0.8)


```

As we can see more clearly now, the pattern is very similar in both types of sequence, though the negative sequences tend to have lower relevances at the middle-to-end area of the sequence. This confirms two of our questions from before (both types having similar distributions, and negative sequences having lower relevances at the end). 

We still have to check if dotted lines (padding sequences) tend to be in sequences with lower relevances.

For the first question, we can divide each type of sequence in two subtypes: those sequences that contain one or more padding characters, and those that do not, and then plot them.

```{r}
with_pad_pos <- nchar(relevancesP[1:nrow(pos_norm),]$rawseq) < 250
nopad_pos_norm <- pos_norm[1 - with_pad_pos,]
pad_pos_norm <- pos_norm[with_pad_pos,]

with_pad_neg <- nchar(relevancesN[1:nrow(neg_norm),]$rawseq) < 250
nopad_neg_norm <- neg_norm[1 - with_pad_neg,]
pad_neg_norm <- neg_norm[with_pad_neg,]

mean_nopad_pos_norm <- colSums(nopad_pos_norm)/nrow(nopad_pos_norm)
mean_pad_pos_norm <- colSums(pad_pos_norm)/nrow(pad_pos_norm)
mean_nopad_neg_norm <- colSums(nopad_neg_norm)/nrow(nopad_neg_norm)
mean_pad_neg_norm <- colSums(pad_neg_norm)/nrow(pad_neg_norm)

plot(type="n", xlim <- c(1,250), c(min(mean_nopad_pos_norm, mean_pad_pos_norm, mean_nopad_neg_norm, mean_pad_neg_norm), max(mean_nopad_pos_norm, mean_pad_pos_norm, mean_nopad_neg_norm, mean_pad_neg_norm)), xlab = "Position of the character in the sequence", ylab="Mean relevance of the character")
lines(mean_nopad_pos_norm, col="blue")
lines(mean_nopad_neg_norm, col="red")
lines(mean_pad_pos_norm, col="black")
lines(mean_pad_neg_norm, col="yellow")
legend("topright", legend=c("Positive no padding", "Negative no padding", "Positive with padding", "Negative with padding"),
       col=c("blue", "red", "black", "yellow"), lty=1, cex=0.8)
  

```

In this plot we can observe that, with some exceptions in certain characters, sequences with no padding characters tend to have much higher relevances in some points of the sequence. This could mean that our padding character is a handicap for the network trainning, and that an alternative could be desirable. 



```

######## RESTOS VIEJOS

```{r, echo=FALSE, eval=FALSE}
## Without normalization
## Every positive sequence plotted
plot(colSums(p_rel[1,]), type="l", ylim=c(min(p_rel), max(p_rel))) 
for (i in 2:(nrow(p_rel))) { 
  lines(colSums(p_rel[i,]))
}

## Sum of every positive sequence
plot(colSums(p_rel),type="l")

## Every negative sequence plotted
plot(colSums(n_rel[1,]), type="l")
for (i in 2:(nrow(n_rel))) {
  lines(colSums(n_rel[i,]))
}

## Sum of every negative sequence
plot(colSums(n_rel),type="l")


```

Problema: Creo que ya no trabajamos con pr y nr, en su lugar usamos relevancesP y relevancesN
```{r}
norm <- normalizeDF(pos_rel, neg_rel)
pos_norm <- norm[[1]]
neg_norm <- norm[[2]]

plot(type="n", xlim <- c(1,250), c(min(pos_norm), max(pos_norm)), xlab = "Position of the character in the sequence", ylab="Relevance of the character")
  for (j in 1:nrow(pos_norm)){
    numX <- max(0, 250 - nchar(pr$rawseq[j]))
    
    lines(colSums(pr_norm[j,1:(250-numX)]), col="blue") # In blue -> Relevances of CAGT
  }
  for (j in 1:nrow(pr_norm)){
    numX <- max(0, 250 - nchar(pr$rawseq[j]))
    if(numX > 0)
      lines(x = (250-numX):250, y = colSums(pr_norm[j,(250-numX):250]), col="blue", lty=2) # In red -> Relevances of X
  }
  
  for (j in 1:nrow(nr_norm)){
    numX <- max(0, 250 - nchar(nr$rawseq[j]))
    
    lines(colSums(nr_norm[j,1:(250-numX)]), col="red") # In blue -> Relevances of CAGT
  }
  for (j in 1:nrow(pr_norm)){
    numX <- max(0, 250 - nchar(nr$rawseq[j]))
    if(numX > 0)
      lines(x = (250-numX):250, y = colSums(nr_norm[j,(250-numX):250]), col="red", lty=2) # In red -> Relevances of X
  }

```

```{r}
## Application of Normalization
pr_norm <- normalizeDF(p_rel)
nr_norm <- normalizeDF(n_rel)

## Every normalized positive sequence plotted
plot(colSums(pr_norm[1,]), type="l", ylim = c(0,1))
for (i in 2:(nrow(pr_norm))) {
  lines(colSums(pr_norm[i,]))
}

## Sum of every normalized positive sequence
plot(colSums(pr_norm),type="l", xlab = "Position of the character in the sequence", ylab="Relevance of the character")

## Every normalized negative sequence plotted
plot(colSums(nr_norm[1,]), type="l", ylim = c(0,1))
for (i in 2:(nrow(nr_norm))) {
  lines(colSums(nr_norm[i,]))
}

## Sum of every normalized negative sequence
plot(colSums(nr_norm),type="l", xlab = "Position of the character in the sequence", ylab="Relevance of the character")


```

```{r}
## Differentiating between real characters and Xs


## Every normalized positive sequence plotted
plot(type="n", xlim <- c(1,250), c(min(pr_norm), max(pr_norm)), xlab = "Position of the character in the sequence", ylab="Relevance of the character")
for (i in 1:nrow(pr_norm)){
  numX <- max(0, 250 - nchar(pr$rawseq[i]))
  
  lines(colSums(pr_norm[i,1:(250-numX)]), col="blue") # In blue -> Relevances of CAGT
}
for (i in 1:nrow(pr_norm)){
  numX <- max(0, 250 - nchar(pr$rawseq[i]))
  if(numX > 0)
    lines(x = (250-numX):250, y = colSums(pr_norm[i,(250-numX):250]), col="red") # In red -> Relevances of X
}

## Every normalized negative sequence plotted
plot(type="n", xlim <- c(1,250), c(min(nr_norm), max(nr_norm)), xlab = "Position of the character in the sequence", ylab="Relevance of the character")
for (i in 1:nrow(nr_norm)){
  numX <- max(0, 250 - nchar(nr$rawseq[i]))
  
  lines(colSums(nr_norm[i,1:(250-numX)]), col="blue") # In blue -> Relevances of CAGT
}
for (i in 1:nrow(nr_norm)){
  numX <- max(0, 250 - nchar(nr$rawseq[i]))
  if(numX > 0)
    lines(x = (250-numX):250, y = colSums(nr_norm[i,(250-numX):250]), col="red") # In red -> Relevances of X
}


```

